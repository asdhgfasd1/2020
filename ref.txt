* 기술 통계란 자료를 그래프나 숫자 등으로 요약하는 통계적 행위 및 관련 방법을 말한다.

    중심경향값(분포의 중심) : 평균, 중위수, 최빈값 ...
    산포도(분포의 퍼짐정도) : 분산, 표준편차, 범위, 사분위수 ...
    분포도(분포의 모양)     : 왜도, 첨도
    도수분포표 등





*추론통계란 모집단에서 샘플링한 표본을 가지고 모집단의 특성을 추론하고 그 결과가 신뢰성이 있는지 검정하는 것이다.



* 가설검정 방법의 종류 *

1. 빈도분석(Frequency) - 카이제곱 검정 - 가설 검정 중 하나

빈도분석은 원천 데이터의 내용들이 도수분포표상에서 어떠한 분포적 특성을 가지고 있는지를 파악하는 데 이용되고 있다. 

이들 분포들의 특성인 통계량들은 

  첫째, 빈도, 상대적 빈도, 누적빈도와 같은 도수분포표로 구성되어 있다. 

  둘째, 최빈값, 중앙값, 산술평균과 같은 중심화 경향을 나타내는 통계량들로 구성되어 있다. 

  셋째, 범위, 평균편차, 분산, 표준편차 등으로 이들은 분산도를 나타내고 있다. 

또한 이같은 특성치들을 하나의 바차트나 히스토그램으로 그래픽 처리하여 나타내는데 있어 빈도분석이 널리 이용되고 있다.

https://ko.wikipedia.org/wiki/%EC%B9%B4%EC%9D%B4%EC%A0%9C%EA%B3%B1_%EB%B6%84%ED%8F%AC



2. T-test 분석  - 가설 검정 중 하나

일반적으로 두 집단의 평균을 비교하는 분석방법에는 크게 Z-검정과 T-검정으로 구분되는데, Z-검정은 모집단의 분산을 알고 있는 경우에 사용된다. 그러나 두 모집단의 분산을 알고 있는 경우는 드물기 때문에 보통 T-검정을 사용하게 된다. 

T-검정은 두 집단 간의 평균의 차이가 통계적으로 유의한지를 파악할 때 이용하는 통계기법이다. 

https://m.blog.naver.com/PostView.nhn?blogId=istech7&logNo=50151098832&proxyReferer=https%3A%2F%2Fwww.google.com%2F



3. 분산분석(Analysis of Variance: ANOVA)  - 가설 검정 중 하나

분산분석은 두 표본 이상의 평균치에 대한 차이를 검정하는 통계기법이다. 이 분산분석을 이용하여 표본들이 동일한 평균을 가진 모집단에서 추출된 것인지의 여부를 추론할 수 있다. 

예를 들면, 분산분석의 이용은 통계학을 수강한 학생들의 점수[종속변수: 비율척도 또는 등간척도]에 대해 학년별[독립변수 : 명목척도]평균의 차이가 있는지를 살펴볼 수 있다. 그리고 이러한 차이가 통계적으로 유의한 것인지를 파악할 필요가 있는데 이같은 상황에서 두 집단 이상의 한 변수에 대한 평균의 차이를 검정하고자 할 때 이용한다. 

https://ko.wikipedia.org/wiki/%EB%B6%84%EC%82%B0_%EB%B6%84%EC%84%9D



4. 상관관계분석(Correlation Analysis) 

상관관계분석은 연구하고자 하는 변수들 간의 관련성을 분석하기 위해 사용한다. 즉, 한 변수가 다른 변수와의 관련성이 있는 지 여부와 관련이 있다면 어느 정도의 관련이 있는지를 알고자 할 때 이용하는 분석기법이다. 상관관계분석은 각각의 변수가 연속형 데이터인 경우에 사용한다. 예를 들면, 소득액과 지출액 간의 관련성 여부라든지, 응답자의 연간 자동차 주행거리와 연령과의 관련성 여부 등을 분석하고자 할 때 이용되는 분석 기법이다.

-- 선행 조건 --

  선형성 : 두 변인 X와 Y의 관계가 직선적인지를 알아보는 것(분포상태)으로, 산점도를 통하여 확인할 수 있다.

  등분산성 : X의 값에 관계없이 Y의 흩어진 정도가 같은 것을 의미한다. 

  정규분포성 : 두 변인의 측정치 분포가 모집단에서 모두 정규분포를 이루는 것이다.

https://ko.wikipedia.org/wiki/%EC%83%81%EA%B4%80_%EB%B6%84%EC%84%9D



5. 회귀분석(Regression Analysis) 

다변량 모집단에서는 모집단을 구성하는 변수들 간에 어떠한 관계가 있는가? 그리고 변수들 간의 인과관계가 존재한다면 이를 어떻게 함수로 표시하는 것이 합리적인가에 대한 문제를 주로 다루고 있다. 

회귀분석은 변수들 중 하나를 종속변수로 나머지를 독립변수로 하여 이들 변수들이 서로 상관관계를 가질 때 독립변수가 변화함에 따라 종속변수가 어떻게 변화 하는가를 규명하는 통계기법이다. 회귀분석은 독립변수의 갯수에 따라 단순회귀분석과 다중 회귀분석으로 구분하고 있다. 

https://support.minitab.com/ko-kr/minitab/18/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/types-of-regression-analyses/



6. 요인분석(Factor Analysis) 

요인분석은 일련의 관측된 변수에 근거하여 직접 관측할 수 없는 요인을 확인하기 위한 것이다. 

예를 들어, 지역사회의 특성을 기술하기 위해서는 지역의 산업화정도, 경제활동, 인구유동성, 가계수입, 주택보유율, 출생률 등 다양한  변수를 사용할 수 있다. 

요인분석은 수 많은 변수들을 몇 가지 요인으로 묶어줌으로써 그 변수의 갯수를 단순화하는 것이 그 목적이다. 

https://namu.wiki/w/%EC%9A%94%EC%9D%B8%20%EB%B6%84%EC%84%9D



주성분분석

https://ko.wikipedia.org/wiki/%EC%A3%BC%EC%84%B1%EB%B6%84_%EB%B6%84%EC%84%9D





^^^ 추론통계 분석용 모델의 비교 ^^^

* Linear Regression

종속변수(Dependent variable) Y와 한 개 이상의 설명변수(Independent Variable, 독립변수) X(들)과의 선형 상관관계를 모델링하는 회귀분석 기법이며 설명변수와 종속 변수가 수치형일 때 사용한다.

종속변수와 독립변수 간의 관계를 함수로 설명하기 때문에 이들 간의 관계를 확인하기에 용이하지만, 종속변수와 독립변수들 간에 선형 관계만을 가정하기 때문에 정확성을 높이는 데는 한계가 있다. 

https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80



* Logistic Regression

종속변수와 설명변수 간의 관계를 함수로 설명하려는 측면은 동일하나 설명변수는 연속형, 종속변수가 범주형 변수일 때 사용한다. 

분류 및 예측에 주로 사용되는 모델이다.

https://ko.wikipedia.org/wiki/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1_%ED%9A%8C%EA%B7%80



* Naive Bayesian Classifier 

자료 특성들 사이의 독립을 가정하여 베이즈 정리를 적용한 분류 알고리즘이다. 

나이브 베이즈는 전통적인 확률론에 기반한 알고리즘은 아니나, 다른 분류 알고리즘과 비교했을 때, 상대적으로 예측력이 높은 것으로 알려져 있다. 

적용 예로 문서의 텍스트를 확인하여 어떤 문서가 어떤 카테고리(스팸여부)에 속하는지, 어떤 메시지(지지/비판/중립 등)를 가지고 있는지 등을 판단하는 작업에 자주 등장하는 알고리즘 이다.

https://ko.wikipedia.org/wiki/%EB%82%98%EC%9D%B4%EB%B8%8C_%EB%B2%A0%EC%9D%B4%EC%A6%88_%EB%B6%84%EB%A5%98


* Support Vector Machine

두 개의 주어진 Class 간에 가장 가까운 거리를 가지는 Support Vector사이의 초평면을 찾아, 새로운 데이터에 Class를 부여하는 분류 방법이다. 주어진 데이터를 이진 분류해 내는 데에 있어 다른 알고리즘 보다 정확도는 뛰어나지만, dataset의 크기나 복잡도에 따라 연산 속도가 느려진다는 단점이 있다. 

주로 텍스트 분석(스팸/비스팸, 긍/부정 등), 손글씨 등의 이미지 인식 등에 자주 사용된다.

https://ko.wikipedia.org/wiki/%EC%84%9C%ED%8F%AC%ED%8A%B8_%EB%B2%A1%ED%84%B0_%EB%A8%B8%EC%8B%A0


* Decision Tree

수치형/범주형 종속 변수에 대한 예측/분류를 위해 사용한다. 

분류 결과를 보다 직관적으로 이해할 수 있다는 점, 계산 방법이 간단하다는 장점이 있지만 과적합화 등으로 인해 다른 모델에 비해 정확도가 낮은 편이다.

https://ko.wikipedia.org/wiki/%EA%B2%B0%EC%A0%95_%ED%8A%B8%EB%A6%AC_%ED%95%99%EC%8A%B5%EB%B2%95



* Random Forest

분류, 회귀 분석 등에 사용되는 앙상블 학습 방법의 일종으로, 훈련 과정에서 구성한 다수의 결정 트리로부터 분류 또는 평균 예측치(회귀 분석)를 출력함으로써 동작한다. 대개의 경우, 다수의 의사결정나무(Decision Tree)를 만든 후 최빈값을 기준으로 예측/분류하는 알고리즘이다. Random 이라는 단어는 붙인 이유는, 데이터 셋에서 Random 으로 추출한 일부 데이터를 가지고 학습을 진행하기 때문이다. Bagging/Bootstrap Aggregating 방법을 사용해 의사결정나무 노드 생성의 Bias를 줄이므로 의사결정나무의 과적합화 문제를 해결할 수 있는 대안으로 사용할 수 있다.

https://ko.wikipedia.org/wiki/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8



* 앙상블(Ensemble)은 여러 알고리즘이 조화를 이루어 결과를 내는 방식이다.

성능이 좀 떨어지는(정확도가 낮은) 알고리즘 하나로는 성능이 안나오다보니 여러개 섞어서 이 중 가장 많이 나온 예측값을 제시한다.



* Bagging(Bootstrap AGGregatING)

Random Forest를 참조해 보면 'bagging은 bootstrap aggregating의 약자로, bootstrap을 통해 조금씩 다른 훈련 데이터에 대해 훈련된 기초 분류기(base learner)들을 결합(aggregating)시키는 방법이다. 

부트스트랩이란, 주어진 훈련 데이터에서 중복을 허용하여 원 데이터와 같은 크기의 데이터를 만드는 과정을 말한다.' 라고 알려져 있다. Ensemble에 부트스트랩을 적용한 것이다.

그럼 Bagging과 Random Forest 의 차이점이 뭘까?

Bagging 은 학습 데이터의 설명 변수(특징)를 모두 사용하지만, Random Forest 는 설명 변수(특징)도 무작위로 추출한다는 차이가 있다. 



* Boosting : 이는 Bagging에 약간의 변화를 준 알고리즘으로 Bagging 이나 Random Forest 에서는 데이터를 Random 하게 추출할 때 각 샘플에 대해서 동일한 확률로 추출하였다. 

n 개의 데이터가 있을 경우, 중복이 허용되기 때문에 각 샘플은 매번 1/n 의 확률로 추출이 된다. 

하지만, Boosting 방식은 첫 번째로 Random 데이터 샘플을 추출할 때를 제외하고는 샘플이 추출 될 확률이 그때 그때 다르다.



간단하게 진행 방식을 보자.

  학습 데이터 셋에서 1/n 확률로 sample 추출

  첫번째로 선택된 알고리즘(학습기)에서 추출된 데이터 셋으로 학습

    - 데이터에 대해 error rate 가 가장 낮은 알고리즘(학습기) 선택

    - 학습 및 예측

    - 학습은 랜덤 데이터 셋, 예측은 추출 대상이 되는 전체 데이터 셋

    - 정확하게 예측한 경우 샘플의 추출 확률을 낮추고, 예측이 틀린 샘플의 경우 추출 확률을 높인다.

  두번째 부터는 이전에 변경된 확률을 기준으로 높이거나 낮춘다.

    - 각 샘플 추출 확률의 총합은 1이 된다.

  2번 부터 다음 알고리즘(학습기)으로 넘어가면서 반복한다.



Boosting 의 대표적인 예는 AdaBoost가 있다.

* AdaBoost : Boosting에서는 모든 알고리즘이 동일한 중요도를 갖고 있지만, AdaBoost는 알고리즘 마다 모두 다른 중요도를 갖고 있다. 중요도는 선택된 알고리즘의 error rate 에 따라 달라진다.

그리고 다음 각 샘플 추출 확률을 결정할 때 이전 알고리즘(학습기)의 중요도에 따라 확률이 변화하는 정도가 달라지게 된다.



* Gradient Boosting Algorithm

Gradient Boosting Algorithm (GBM)은 회귀분석 또는 분류 분석을 수행할 수 있는 예측모형이며 예측모형의 앙상블 방법론 중 부스팅 계열에 속하는 알고리즘이다. 

Gradient Boosting은 Tabular format 데이터 (엑셀형태와 같이 X-Y Grid로 되어 있는 데이터)에 대한 예측에서 엄청난 성능을 보여주고, 머신러닝 알고리즘 중에서도 가장 예측 성능이 높다고 알려져 있다. 

그렇기 때문에 Gradient Boosting Algorithm을 구현한 패키지들이 많다. LightGBM, CatBoost, XGBoost 같은 파이썬 패키지들이 모두 Gradient Boosting을 구현한 패키지들이다. 

GBM은 계산량이 상당히 많이 필요한 알고리즘이기 때문에, 이를 하드웨어 효율적으로 구현하는 것이 필요한데, 위 패키지들은 모두 GBM을 효율적으로 구현하려 한 패키지들이라고 볼 수 있다.



* XGBoost(eXtreme Gradient Boosting)는 병렬처리와 최적화를 장점으로 내세우는 Gradient boosting 알고리즘 으로 Kaggle 대회 등에서 인기를 얻으며 많은 관심을 끈 방법론이다. 이는 준수한 성능 및 속도와 information gain 기반 importance 산출 기능을 제공해 많은 사랑을 받고 있다.

https://apple-rbox.tistory.com/6

https://brunch.co.kr/@snobberys/137



* k-NN 

지도 학습(Supervised Learning)의 한 종류로 레이블이 있는 데이터를 사용하여 분류 작업을 하는 알고리즘으로 

데이터로부터 거리가 가까운 k개의 다른 데이터의 레이블을 참조하여 분류한다. 

데이터 간에 거리를 측정할 때는 주로 유클리디안 거리 계산법을 사용하는데, 벡터의 크기가 커지면 계산이 복잡해진다.

  - 장점으로 알고리즘이 간단하여 구현하기 쉽고, 수치 기반 데이터 분류 작업에서 성능이 좋다

  - 단점으로는 학습 데이터의 양이 많으면 분류 속도가 느려진다.



참고 : K-Means++ 알고리즘은 최초의 중심값을 설정하기 위한 알고리즘이다.

 1) 중심값을 저장할 집합  M 준비

 2) 일단 하나의 중심μ0 를 랜덤하게 선택하여 M에 넣는다.

 3) M 에 속하지 않는 모든 샘플 xi 에 대해 거리 d(M,xi)를 계산. d(M,xi)는 M 안의 모든 샘플 μk에 대해 d(μk,xi)를 계산하여 가장 작은 값 선택

 4) d(M,xi) 에 비례한 확률로 다음 중심 μ를 선택.

 5) K 개의 중심을 선택할 때까지 반복

 6) K-Means 알고리즘 사용

https://ko.wikipedia.org/wiki/K-%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98





* 인공신경망

신경망(neural network) 모형은 기저 함수(basis function)도 모수(parameter) 값에 의해 변화할 수 있는 

적응형 기저 함수 모형(adaptive basis function model)이며 구조적으로는 여러 개의 Perceptron을 묶어놓은 형태이므로 MLP(multi-layer perceptron)라고도 불린다.

ANN에서 진화되어서 파생되어 나온 개념이라고 볼 수 있다. 

https://ko.wikipedia.org/wiki/%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D




@ K-means Clustering

비지도 학습의 대표적인 분석 방법으로 주어진 데이터를 유사한 K개의 군집으로 묶는 알고리즘이다. 

군집을 나누는 방법에 따라 여러 종류로 구분된다. 데이터에 대한 이해 단계인 EDA(Exploratory Data Analysis)단계에서 부터 고객 세그멘테이션, 이미지 분할 등에 광범위하게 적용 가능하다.

하지만, K값을 사전에 지정해 주어야 한다는 점, 이상치에 민감하게 반응하는 점 및 구형이 아닌 군집을 찾는데는 적절하지 않다는 단점이 있다.

https://ko.wikipedia.org/wiki/K-%ED%8F%89%EA%B7%A0_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98



**  딥러닝 프레임워크 ** 
* TensorFlow

구글의 딥러닝 프로젝트 중 최근 가장 주목받는 프로젝트는 텐서플로우다. 

이는 구글이 만든 2세대 머신러닝 시스템으로 문자, 이미지, 음성, 비디오 등 다양하고 많은 데이터를 처리할 수 있는 것이  특징이다. 모델을 훈련할 때는 주로 CPU 보다는 GPU, TPU를 이용하여 속도를 증진시킬 수 있다.

핵심 기술은 C++로 작성됐고, 파이썬, R, Java 등에서 사용할 수 있다.

https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/



* Pytorch

두 가지를 목표로 하는 과학 연산 및 딥러닝을 위한 파이썬 패키지 .

  1. GPU 연산을 사용하기 위한 Numpy 대체물이다.

  2. 속도와 유연성을 극대로 한 딥러닝 연구 플랫폼이다.

즉, 기존 수치 연산 라이브러리인 Numpy를 딥러닝 연구 목적으로 GPU에서도 사용 가능하며, 속도와 유연성을 극대화한 딥러닝 플랫폼이다.



-----------------------------------------------



두 그룹 간의 비교

  연속값 : 평균값의 차이를 위한 t검정

  이산값 : 집계표의 기술을 바탕으로 한 카이제곱 검정



여러 그룹 간의 비교

 연속값 : 평균값의 차이로 분산 분석

 이산값 : 집계표의 기술을 바탕으로 카이제곱 검정



연속값의 크기로 비교

 연속값 : 회귀 분석

 이산값 : 로지스틱 회귀 분석



복수의 요인으로 동시 비교

 연속값 : 다중 회귀 분석

 이산값 : 로지스틱 회귀 분석



분석알고리즘의 종류 및 장단점 https://docs.microsoft.com/ko-kr/azure/machine-learning/studio/algorithm-choice
